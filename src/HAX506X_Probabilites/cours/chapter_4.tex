\subsection*{Contexte}

On a vu que si \({(X_n)}_{n\in\bb N}\) était une suite
i.i.d.\ de variables aléatoires réelles intégrables,
alors
\begin{equation*}
    \frac{X_1 + \cdots + X_n}{n} \cvpsn \bb E[X_1]
\end{equation*}

\subsection*{Question}

Que dire de l'écart entre \(\frac{X_1 + \cdots + X_n}{n}\) et \(\bb E[X_1]\)?

\section{Convergence en loi}

\subsection*{Notation}

Dans cette section, \(C_b^0(\bb R)\) l'ensemble
des fonctions \(h:\bb R\to\bb R\) continues bornées
munies de la norme
\begin{equation*}
    \nn{h}_{\infty} = \sup_{x\in\bb R} |h(x)|
\end{equation*}

\subsection{Définition et premiers exemples}

\begin{definition}
    Soit \({(X_n)}_{n\geq 1}\) une suite de variables aléatoires réelles
    et \(X\) une variable aléatoire réelle. On dit que \({(X_n)}_{n\geq 1}\)
    \defemph{converge en loi} vers \(X\) si pour tout \(h\in C_b^0(\bb R)\),
    on a
    \begin{equation*}
        \bb E[h(X_n)] \cvn \bb E[h(X)]
    \end{equation*}
    On note alors
    \begin{equation*}
        X_n \cvlawn X
    \end{equation*}
    ou bien
    \begin{equation*}
        X_n \cvdn X
    \end{equation*}
\end{definition}

\begin{remark}\,
    \begin{enumerate}
        \item On peut étendre cette définition à des vecteurs aléatoires
        en prenant des fonctions \(h:\bb R^d\to\bb R\) continues bornées.

        \item Cette notion de convergence ne fait intervenir que la loi
        des variables aléatoires considérées. Dans la limite, on peut donc
        remplacer \(X\) par n'importe quelle variable aléatoire \(Y\) de
        même loi.\\
        Si \(\mu\) désigne la loi de \(X\), on notera alors
        \begin{equation*}
            X_n \cvlawn \mu
        \end{equation*}
    \end{enumerate}
\end{remark}

\begin{example}
    Soit \({(X_n)}_{n\geq 1}\) une suite de variables aléatoires
    telles que pour tout \(n\geq 1\), \(X_n\) suit une loi uniforme
    sur \(\left\{0,\frac1n,\frac2n,\ldots,\frac{n-1}{n},1\right\}\).
    % graph of X1,X2 and X3 as a line with points on every i/n:
    % X1: 0, 1
    % X2: 0, 1/2, 1
    % X3: 0, 1/3, 2/3, 1
    \begin{center}
        \begin{tikzpicture}
            % lines
            \draw (-2.5,2) -- (2.5,2);
            \draw (-2.5,1) -- (2.5,1);
            \draw (-2.5,0) -- (2.5,0);

            % 0s
            \node[below] at (-2.25,2) {\(0\)};
            \node[below] at (-2.25,1) {\(0\)};
            \node[below] at (-2.25,0) {\(0\)};

            % 1s
            \node[below] at (2.25,2) {\(1\)};
            \node[below] at (2.25,1) {\(1\)};
            \node[below] at (2.25,0) {\(1\)};

            % bonus points for X1
            %%

            % points for X2
            \node[below] at (0,1) {\(\frac12\)};

            % points for X3 %-2.5+1.6666666666666667 = -0.8333333333333333
            \node[below] at (-0.8333,0) {\(\frac13\)};
            \node[below] at (0.8333,0) {\(\frac23\)};

            % probabilities for all points above
            \node[above] at (-2.25,2) {\(\frac12\)};
            \node[above] at (-0.8333,0) {\(\frac14\)};
            \node[above] at (0.8333,0) {\(\frac14\)};
            \node[above] at (2.25,2) {\(\frac12\)};




        \end{tikzpicture}
    \end{center}

    La loi de \(X_n\) est donnée par
    \begin{equation*}
        \bb P(X_n = k) = \frac1{n+1} \quad \text{pour } k=0,1,\ldots,n
    \end{equation*}
    Soit \(h\in C_b^0(\bb R)\). On a
    \begin{equation*}
        \begin{aligned}
            \bb E[h(X_n)] 
            &= \sum_{k=0}^n h\left(\frac kn\right) \frac1{n+1}\\
            &= \frac1{n+1} \sum_{k=0}^n h\left(\frac kn\right)\\
            &\cvn \int_0^1 h(x) \der x
        \end{aligned}
    \end{equation*}
    par convergence des sommes de Riemann. On considère une variable aléatoire
    \(X\) suivant une loi uniforme sur \([0,1]\). On a alors
    \begin{equation*}
        \bb E[h(X)] = \int_0^1 h(x) \der x
    \end{equation*}
    Donc pour tout \(h\in C_b^0(\bb R)\), on a
    \begin{equation*}
        \bb E[h(X_n)] \cvn \bb E[h(X)]
    \end{equation*}
    et donc
    \begin{equation*}
        X_n \cvlawn X
    \end{equation*}
    Pkus simplement, on écrit
    \begin{equation*}
        X_n \cvlawn \mathcal U([0,1])
    \end{equation*}
\end{example}

\begin{example}
    Soit \({(X_n)}_{n\geq 1}\) une suite de variables aléatoires
    telle que \(X_n\sim\mathcal N(0,\sigma_n^2)\) avec \(\sigma_n> 0\)
    et \(\sigma_n\cvn 0\). On a alors
    \begin{equation*}
        \begin{aligned}
            \bb E[h(X_n)]
            &= \int_{\bb R} h(x) \frac{e^{-\frac{x^2}{2\sigma_n^2}}}{\sqrt{2\pi}\sigma_n}  \der x\\
            \smol{\(y=\frac{x}{\sigma_n}\)}&= \int_{\bb R} \underbrace{h(\sigma_n y) \frac{e^{-\frac{y^2}{2}}}{\sqrt{2\pi}}}_{g_n(y)\cvn h(0) \frac{e^{-\frac{y^2}{2}}}{\sqrt{2\pi}}} \der y\\
        \end{aligned}
    \end{equation*}
    Comme \(|g_n(y)|\leq \nn{h}_{\infty} \frac{e^{-\frac{y^2}{2}}}{\sqrt{2\pi}}\), on
    applique le théorème de convergence dominée pour obtenir
    \begin{equation*}
        \bb E[h(X_n)] \cvn \int_{\bb R} h(0) \frac{e^{-\frac{y^2}{2}}}{\sqrt{2\pi}} \der y= h(0)
    \end{equation*}
    Soit \(X\) de loi \(\delta_n\). On a montré que pour tout \(h\in C_b^0(\bb R)\),
    \begin{equation*}
        \bb E[h(X_n)] \cvn \bb E[h(X)]=h(0)
    \end{equation*}
    donc
    \begin{equation*}
        X_n \cvlawn X
    \end{equation*}
    ce qu'on réécrit
    \begin{equation*}
        X_n \cvlawn \delta_0
    \end{equation*}
\end{example}

\subsection{Deux cas particuliers}

\subsubsection{Loi sur \(\bb N\)}

\begin{proposition}
    Une suite de variables aléatoires \({(X_n)}_{n\geq 1}\) converge 
    vers une variable aléatoire \(X\) à valeurs dans \(\bb N\) si et seulement
    si pour tout \(k\in\bb N\), on a
    \begin{equation*}
        \bb P(X_n = k) \cvn \bb P(X = k)
    \end{equation*}
\end{proposition}

\begin{proof}\,
    \begin{itemize}[\ptr{}]
        \item Sens direct:\\
        Supposons que \(X_n \cvlawn X\). Soit \(k\in\bb N\). On sait que
        pour tout \(h\in C_b^0(\bb R)\), on a
        \begin{equation*}
            \begin{aligned}
                \bb E[h(X_n)]
                &= \sum_{j=0}^{+\infty} h(j) \bb P(X_n = j)\\
                &\cvn \bb E[h(X)]\\
                &= \sum_{j=0}^{+\infty} h(j) \bb P(X = j)
            \end{aligned}
        \end{equation*}
        On prend alors \(h\) telle que \(h(k) = 1\) et \(h(j) = 0\) pour
        \(j\neq k\).
        % graph of h 0 everywhere except at k where it is 1, forms a 
        % triangle with base 2 centered at k, height 1
        %\begin{center}
        %    \begin{tikzpicture}
        %        % axes
        %        \draw[->] (-2,0) -- (2,0);
        %        \draw[->] (0,-0.5) -- (0,1.5);
%
        %        % h
        %        \draw (-1,0) -- (1,1) -- (1,0);
%
        %        % labels
        %        \node[below] at (1,0) {\(k\)};
        %        \node[left] at (0,1) {\(1\)};
        %    \end{tikzpicture}
        %\end{center}
        Ainsi,
        \begin{equation*}
            \bb E[h(X_n)] = \sum_{j=0}^{+\infty} h(j) \bb P(X_n = j) = \bb P(X_n = k) 
        \end{equation*}
        et de même pour \(X\), donc
        \begin{equation*}
            \bb P(X_n = k) \cvn \bb P(X = k)
        \end{equation*}

        \item Réciproque vue plus tard.
    \end{itemize}
\end{proof}

\begin{example}
    Approximation binomiale de la loi de Poisson. Soit \({(X_n)}_{n\geq 1}\)
    une suite de variables aléatoires de loi \(X_n\sim\mathcal B(n,p_n)\) avec
    \(n p_n\cvn \theta>0\). Montrons que \(X_n \cvlawn X\) où \(X\sim\mathcal P(\theta)\).

    Soit \(k\in\bb N\). On a
    \begin{equation*}
        \begin{aligned}
            \bb P(X_n = k)
            &= \binom{n}{k} p_n^k {(1-p_n)}^{n-k}\\
            &= \frac{n!}{k!(n-k)!} p_n^k {(1-p_n)}^{n-k}\\
            &= \frac{n(n-1)\cdots(n-k+1)}{k!} {\left(\frac{p_n}{1-p_n}\right)}^k {(1-p_n)}^n\\
            &\cvn \frac{\theta^k}{k!} e^{-\theta}
        \end{aligned}
    \end{equation*}
    Bref, on obtient
    \begin{equation*}
        \bb P(X_n = k) \cvn \frac{\theta^k}{k!} e^{-\theta}= \bb P(X = k)
    \end{equation*}
    donc
    \begin{equation*}
        X_n \cvlawn X
    \end{equation*}
    ou encore
    \begin{equation*}
        X_n \cvlawn \mathcal P(\theta)
    \end{equation*}
\end{example}

\subsubsection{Loi à densité sur \(\bb R\)}

\begin{proposition}
    Soit \({(X_n)}_{n\geq 1}\) une suite de variables aléatoires
    telles que \(X_n\) a pour densité \(f_n\) et \(X\) 
    une variable aléatoire de densité \(f\). Si
    \begin{equation*}
        \quad f_n \cvn f,\quad \lambda\text{-p.p.}
    \end{equation*}
    alors
    \begin{equation*}
        X_n \cvlawn X
    \end{equation*}
\end{proposition}

\begin{proof}
    On veut montrer que pour tout \(h\in C_b^0(\bb R)\), on a
    \begin{equation*}
        \begin{aligned}
            \bb E[h(X_n)]
            &= \int_{\bb R} h(x) f_n(x) \der \lambda_1(x)\\
            &\cvn \bb E[h(X)]\\
            &= \int_{\bb R} h(x) f(x) \der \lambda_1(x)
        \end{aligned}
    \end{equation*}
    le théorème de convergence dominée ne s'applique pas.

    On applique le lemme de Fatou à la suite de fonctions
    positives mesurables
    \begin{equation*}
        g_n(x) = \min(f_n(x),f(x)) = \frac{f_n(x)+f(x)-|f_n(x)-f(x)|}{2}
    \end{equation*}
    On a
    \begin{equation*}
        \int_{\bb R} \liminf_{n\to+\infty} g_n(x) \der \lambda_1(x) \leq \liminf_{n\to+\infty} \int_{\bb R} g_n(x) \der \lambda_1(x)
    \end{equation*}
    Par ailleurs, on a
    \begin{equation*}
        \begin{aligned}
            \int_{\bb R} g_n(x) \der \lambda_1(x)
            &= \frac12 \int_{\bb R} f_n(x) \der \lambda_1(x) + \frac12 \int_{\bb R} f(x) \der \lambda_1(x) - \frac12 \int_{\bb R} |f_n(x)-f(x)| \der \lambda_1(x)\\
            &= 1 - \frac12 \int_{\bb R} |f_n(x)-f(x)| \der \lambda_1(x)
        \end{aligned}
    \end{equation*}
    Bref, on a
    \begin{equation*}
        0\leq \liminf_{n\to+\infty} \int_{\bb R} |f_n(x)-f(x)| \der \lambda_1(x)
    \end{equation*}
    Ainsi, on a
    \begin{equation*}
        \limsup_{n\to+\infty} \int_{\bb R} |f_n(x)-f(x)| \der \lambda_1(x) \leq 0
    \end{equation*}
    et donc
    \begin{equation*}
        \lim_{n\to+\infty} \int_{\bb R} |f_n(x)-f(x)| \der \lambda_1(x) = 0
    \end{equation*}
    Par suite, on en déduit que
    \begin{equation*}
        \n{\bb E[h(X_n)] - \bb E[h(X)]} \leq \nn{h}_{\infty} \int_{\bb R} |f_n(x)-f(x)| \der \lambda_1(x) \cvn 0
    \end{equation*}
\end{proof}

\begin{example}
    Si \(X\sim E(\theta_n)\) avec \(\theta_n\cvn \theta\), alors 
    les densités de \(X_n\) vérifient
    \begin{equation*}
        f_n(x) = \theta_n e^{-\theta_n x} \cvn \theta e^{-\theta x} = f(x)
    \end{equation*}
    où \(f\) est la densité d'une loi exponentielle de paramètre \(\theta\).
    Donc
    \begin{equation*}
        X_n \cvlawn \mathcal E(\theta)
    \end{equation*}
\end{example}

\subsection[Lien avec les autres modes de convergence]