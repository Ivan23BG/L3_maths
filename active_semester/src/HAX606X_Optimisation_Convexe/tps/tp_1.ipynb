{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1: Méthodes d'optimisation en 1D\n",
    "\n",
    "## 1. Méthode de la dichotomie\n",
    "\n",
    "1. Quelle équation souhaite-t-on résoudre pour notre problème d'optimisation ? Quelles conditions doit-on vérifier pour $f$ pour appliquer la méthode de dichotomie ?\n",
    "\n",
    "2. Ecrire l'algorithme de la méthode de la dichotomie pour trouver le minimum de la fonction $f(x) = x^2 - 2 \\sin(x)$ sur $[0, 2]$ avec une précision de $10^{-5}$.\n",
    "Comment obtient-on le nombre d'itérations à partir de la précision ?\n",
    "\n",
    "3. Comparer votre code avec l’implémentation de `scipy.optimize.bisect`. Que remarquez-vous ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. On souhaite résoudre l'équation $f'(x) = 0$ pour trouver le minimum de $f$. Pour appliquer la méthode de dichotomie, $f$ doit être unimodale sur $[a, b]$, continue et vérifier $f(a) * f(b) < 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. On commence par importer les librairies nécessaires et définir la fonction $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import bisect\n",
    "from scipy.optimize import golden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to minimise\n",
    "def f(x):\n",
    "    return x**2 - 2 * np.sin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Ensuite on définit la fonction `dichotomie` qui prend en argument la fonction $f$, les bornes $a$ et $b$ de l'intervalle sur lequel on cherche le minimum, et la précision $\\varepsilon$. Puis on l'applique sur notre fonction $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dichotomous search algorithm\n",
    "def dichotomie(f, a, b, epsilon):\n",
    "    while b - a > epsilon:\n",
    "        c = (a + b) / 2\n",
    "        if f(a) * f(c) < 0:\n",
    "            b = c\n",
    "        else:\n",
    "            a = c\n",
    "    return (a + b) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the search conditions\n",
    "a = 0\n",
    "b = 2\n",
    "epsilon = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the derivative of the function\n",
    "def df(x):\n",
    "    return 2 * x - 2 * np.cos(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum of the function is at x = 0.7390861511230469\n"
     ]
    }
   ],
   "source": [
    "# Apply the search algorithm to the function\n",
    "x_min = dichotomie(df, a, b, epsilon)\n",
    "print('The minimum of the function is at x =', x_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Pour obtenir le nombre d'itérations à partir de la précision, on utilise la formule $$n = \\frac{\\log(\\frac{b - a}{\\varepsilon})}{\\log(2)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. On remarque que la méthode de dichotomie de `scipy.optimize.bisect` donne le même résultat. Un test plus avancé avec des fonctions plus complexes pourrait montrer des différences en termes de performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum of the function is at x = 0.7390861511230469\n"
     ]
    }
   ],
   "source": [
    "# Comparison with the scipy library\n",
    "x_min_bisect = bisect(df, a, b, rtol=epsilon)\n",
    "print('The minimum of the function is at x =', x_min_bisect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Méthode de Newton\n",
    "\n",
    "4. Quelle condition doit vérifier $f$ pour appliquer la méthode de Newton pour le problème d'optimisation ? Comment va être formulé l'itéré de Newton dans ce cas ?\n",
    "\n",
    "5. Ecrire l'algorithme de Newton dans ce cas et l'appliquer à la fonction $f(x) = x^2 - 2 \\sin(x)$ avec $x_0 = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Pour appliquer la méthode de Newton, $f$ doit être deux fois dérivable et $f''$ doit être continue. L'itéré de Newton est donné par la formule $$x_{n+1} = x_n - \\frac{f'(x_n)}{f''(x_n)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. On définit la fonction `newton` qui prend en argument la dérivée première `df`, la dérivée seconde `df2`, la valeur initiale `x0` et la précision `epsilon`. Puis on l'applique sur notre fonction $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Newton search algorithm\n",
    "def newton(df, df2, x0, epsilon):\n",
    "    x = x0\n",
    "    while abs(df(x)) > epsilon:\n",
    "        x = x - df(x) / df2(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the search conditions\n",
    "x0 = 1\n",
    "epsilon = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to minimise and its derivatives\n",
    "def f(x):\n",
    "    return x**2 - 2 * np.sin(x)\n",
    "\n",
    "\n",
    "def df(x):\n",
    "    return 2 * x - 2 * np.cos(x)\n",
    "\n",
    "\n",
    "def df2(x):\n",
    "    return 2 + 2 * np.sin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum of the function is at x = 0.739085133385284\n"
     ]
    }
   ],
   "source": [
    "# Apply the search algorithm to the function\n",
    "x_min = newton(df, df2, x0, epsilon)\n",
    "print('The minimum of the function is at x =', x_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. On remarque que la méthode de Newton converge plus rapidement que la méthode de dichotomie. Cependant, elle nécessite des conditions plus restrictives sur la fonction $f$. Dans le cas de la dichotomie, $f$ doit être unimodale, tandis que pour Newton, $f$ doit être deux fois dérivable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Méthode de la section dorée\n",
    "\n",
    "6. Ecrire l’algorithme et l’appliquer à la fonction $f(x) = x^2 - 2 \\sin(x)$.\n",
    "\n",
    "7. Comparer votre code avec l’implémentation de `scipy.optimize.golden`.\n",
    "\n",
    "8. Comparer les 3 méthodes pour $f(x) = -\\frac{1}{x} + \\cos(x)$ sur $[a, b] = [2; 4]$ ou pour $x_0 = 2.5$ au niveau du nombre d’itérations et du temps de calcul. Représenter le graphique de la fonction en plaçant les résultats des itérations successives de Newton."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. On définit la fonction `golden` qui prend en argument la fonction $f$, les bornes $a$ et $b$ de l'intervalle sur lequel on cherche le minimum, et la précision $\\varepsilon$. Puis on l'applique sur notre fonction $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the golden search algorithm\n",
    "def golden(f, a, b, epsilon):\n",
    "    rho = (1 + np.sqrt(5)) / 2\n",
    "    x1 = 1 / rho * a + (1 - 1 / rho) * b\n",
    "    x2 = (1 - 1 / rho) * a + 1 / rho * b\n",
    "    while b - a > epsilon:\n",
    "        if f(x1) < f(x2):\n",
    "            b = x2\n",
    "            x2 = x1\n",
    "            x1 = 1 / rho * a + (1 - 1 / rho) * b\n",
    "        else:\n",
    "            a = x1\n",
    "            x1 = x2\n",
    "            x2 = (1 - 1 / rho) * a + 1 / rho * b\n",
    "    return (a + b) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the search conditions\n",
    "a = 0\n",
    "b = 2\n",
    "epsilon = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum of the function is at x = 0.7390861927011781\n"
     ]
    }
   ],
   "source": [
    "# Apply the search algorithm to the function\n",
    "x_min = golden(f, a, b, epsilon)\n",
    "print('The minimum of the function is at x =', x_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. On remarque que la méthode de la section dorée de `scipy.optimize.golden` donne le même résultat. Un test plus avancé avec des fonctions plus complexes pourrait montrer des différences en termes de performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum of the function is at x = 0.7390861927011781\n"
     ]
    }
   ],
   "source": [
    "# Comparison with the scipy library\n",
    "x_min_golden = golden(f, a, b, epsilon)\n",
    "print('The minimum of the function is at x =', x_min_golden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. On définit la fonction $f$ et on applique les 3 méthodes sur $f(x) = -\\frac{1}{x} + \\cos(x)$ avec $[a, b] = [2; 4]$ ou $x_0 = 2.5$. On compare les 3 méthodes en termes de nombre d'itérations et de temps de calcul. On représente le graphique de la fonction en plaçant les résultats des itérations successives de Newton. Pour cela on va devoir réécrire les fonctions `dichotomie`, `newton` et `golden` pour qu'elles retournent le nombre d'itérations et les valeurs successives de $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to minimise and its derivatives\n",
    "def f(x):\n",
    "    return - 1 / x + np.cos(x)\n",
    "\n",
    "def df(x):\n",
    "    return 1 / x**2 + np.sin(x)\n",
    "\n",
    "def df2(x):\n",
    "    return -2 / x**3 + np.cos(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the search conditions\n",
    "a = 2\n",
    "b = 4\n",
    "x0 = 2.5\n",
    "epsilon = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine the search algorithms to return the number of iterations\n",
    "def dichotomie(f, a, b, epsilon):\n",
    "    n = 0\n",
    "    while b - a > epsilon:\n",
    "        c = (a + b) / 2\n",
    "        if f(a) * f(c) < 0:\n",
    "            b = c\n",
    "        else:\n",
    "            a = c\n",
    "        n += 1\n",
    "    return (a + b) / 2, n\n",
    "\n",
    "\n",
    "def newton(df, df2, x0, epsilon):\n",
    "    x = x0\n",
    "    n = 0\n",
    "    while abs(df(x)) > epsilon:\n",
    "        x = x - df(x) / df2(x)\n",
    "        n += 1\n",
    "    return x, n\n",
    "\n",
    "\n",
    "def golden(f, a, b, epsilon):\n",
    "    rho = (1 + np.sqrt(5)) / 2\n",
    "    x1 = 1 / rho * a + (1 - 1 / rho) * b\n",
    "    x2 = (1 - 1 / rho) * a + 1 / rho * b\n",
    "    n = 0\n",
    "    while b - a > epsilon:\n",
    "        if f(x1) < f(x2):\n",
    "            b = x2\n",
    "            x2 = x1\n",
    "            x1 = 1 / rho * a + (1 - 1 / rho) * b\n",
    "        else:\n",
    "            a = x1\n",
    "            x1 = x2\n",
    "            x2 = (1 - 1 / rho) * a + 1 / rho * b\n",
    "        n += 1\n",
    "    return (a + b) / 2, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum of the function is at x = 3.237163543701172 after 18 iterations\n",
      "The minimum of the function is at x = 3.2371648550248726 after 3 iterations\n",
      "The minimum of the function is at x = 3.0326441743922623 after 26 iterations\n"
     ]
    }
   ],
   "source": [
    "# Apply the search algorithms to the function\n",
    "x_min_dicho, n_dicho = dichotomie(df, a, b, epsilon)\n",
    "print('The minimum of the function is at x =', x_min_dicho, 'after', n_dicho, 'iterations')\n",
    "\n",
    "x_min_newt, n_newt = newton(df, df2, x0, epsilon)\n",
    "print('The minimum of the function is at x =', x_min_newt, 'after', n_newt, 'iterations')\n",
    "\n",
    "x_min_gold, n_gold = golden(f, a, b, epsilon)\n",
    "print('The minimum of the function is at x =', x_min_gold, 'after', n_gold, 'iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphical representation of the different search algorithms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
