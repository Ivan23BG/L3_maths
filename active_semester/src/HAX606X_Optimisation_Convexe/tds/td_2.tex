% --- Consignes exo 1
\begin{td-exo}[Quelques révisions]\,
    \begin{enumerate}[label=(\alph*)] % chktex 36
        \item Soit \(A\in S_n(\bb R)\), montrer que
        \begin{equation*}
            A\in S_n^+(\bb R) \iff \spect(A) \sub \bb R_+
        \end{equation*}

        \item Soit \(A\in S_n(\bb R)\), montrer que
        \begin{equation*}
            A^2\in S_n^+(\bb R)
        \end{equation*}

        \item Soit \(A\in S_n(\bb R)\). On note \(\lambda_1\leq \cdots \leq \lambda_n\) ses valeurs propres.
        Montrer que pour tout \(x\in \bb R^n\)
        \begin{equation*}
            x^T A x \geq \lambda_1 \nn{x}^2
        \end{equation*}

        \item Soit \(A\in S_n^+(\bb R)\). Montrer qu'il existe \(B\in S_n^+(\bb R)\) tel que
        \(A = B^2\).
    \end{enumerate}
\end{td-exo}
% --- Solutions exo 1
\iftoggle{showsolutions}{
    \begin{td-sol}\, % 1
        \begin{enumerate}[label=(\alph*)] % chktex 36
            \item Montrons d'abord le sens direct. On suppose que \(A\in S_n^+(\bb R)\). Soit
            \(\lambda\in\bb R\) une valeur propre de \(A\) et \(v\) le vecteur propre associé.
            On a alors
            \begin{equation*}
                v^T A v = \lambda \nn{v}^2 \geq 0
            \end{equation*}
            car \(A\in S_n^+(\bb R)\). Donc \(\lambda \geq 0\).

            Réciproquement, supposons que \(\spect(A) \sub \bb R_+\), elle est alors
            diagonalisable dans une base orthonormée. Soit \(v_1, \ldots, v_n\) une base
            orthonormée de vecteurs propres de \(A\). On a alors
            \begin{equation*}
                v = \sum_{i=1}^n \alpha_i v_i
            \end{equation*}
            et
            \begin{equation*}
                v^T A v = v^T \sum_{i=1}^n \lambda_i v_i = \sum_{i=1}^n \lambda_i \alpha_i^2 \geq 0
            \end{equation*}
            car \(\lambda_i \geq 0\). Donc \(A\in S_n^+(\bb R)\).

            \item Soit \(A\in S_n(\bb R)\). Alors \(A\) est diagonalisable et on note
            \(A = PDP^{-1}\) avec \(D\) diagonale et \(P\) inversible. On a alors
            \begin{equation*}
                A^2 = PD^2P^{-1}
            \end{equation*}
            et \(D^2\) est diagonale avec des éléments \(\lambda_i^2\). Donc tous ses 
            éléments sont positifs \(\spect(A^2)\sub\bb R^+\) et donc \(A^2\in S_n^+(\bb R)\) (voir question précédente).

            \item Soit \(A\in S_n(\bb R)\). On a \(A = PDP^{-1}\) avec \(D\) diagonale et \(P\) inversible.
            On a alors
            \begin{equation*}
                x^T A x = x^T PDP^{-1} x = {(P^{-1}x)}^T D (P^{-1}x) = \sum_{i=1}^n \lambda_i \alpha_i^2
            \end{equation*}
            avec \(\alpha = P^{-1}x\). Comme \(\lambda_1 \leq \cdots \leq \lambda_n\), on a
            \begin{equation*}
                x^T A x = \sum_{i=1}^n \lambda_i \alpha_i^2 \geq \lambda_1 \sum_{i=1}^n \alpha_i^2 = \lambda_1 \nn{x}^2
            \end{equation*}

            \item Soit \(A\in S_n^+(\bb R)\) qu'on écrit \(A = PDP^{-1}\) avec \(D\) diagonale et \(P\) inversible.
            On pose
            \begin{equation*}
                B = P \begin{pmatrix}
                    \sqrt{\lambda_1} & & 0\\
                    & \ddots & \\
                    0 & & \sqrt{\lambda_n}
                \end{pmatrix} P^{-1}
            \end{equation*}
            qui existe car \(\lambda_i\geq 0\) et donc \(B\) est bien définie. On a alors
            \begin{equation*}
                B^2 = P \begin{pmatrix}
                    \sqrt{\lambda_1} & & 0\\
                    & \ddots & \\
                    0 & & \sqrt{\lambda_n}
                \end{pmatrix} P^{-1} P \begin{pmatrix}
                    \sqrt{\lambda_1} & & 0\\
                    & \ddots & \\
                    0 & & \sqrt{\lambda_n}
                \end{pmatrix} P^{-1} = P \begin{pmatrix}
                    \lambda_1 & & 0\\
                    & \ddots & \\
                    0 & & \lambda_n
                \end{pmatrix} P^{-1} = A
            \end{equation*}
        \end{enumerate}
    \end{td-sol}
}{}

% --- Consignes exo 2
\begin{td-exo}
    On définit la fonction 
    \begin{equation*}
        \begin{aligned}
            J\from &\bb R^2 \to \bb R \\
            &(x,y) \mapsto y^4 - 3xy^2 + x^2
        \end{aligned}
    \end{equation*}
    \begin{enumerate}[label=(\alph*)] % chktex 36
        \item Déterminer les points critiques de \(J\).

        \item Soit \(d = (d_1, d_2)\in\bb R^2\). Utiliser l'application
        \begin{equation*}
            \begin{aligned}
                &\bb R\to \bb R\\
                &t \mapsto J(t d_1, t d_2),
            \end{aligned}
        \end{equation*}
        pour montrer que \((0, 0)\) est un minimum local le long de toute droite passant par \((0, 0)\).

        \item Le point \((0, 0)\) est-il un minimum local de la restriction de \(J\)
        à la parabole d'équation \(x = y^2\)?

        \item Calculer \(J''\) et déterminer la nature du point critique \((0, 0)\).
    \end{enumerate}
\end{td-exo}
% --- Solutions exo 2
\iftoggle{showsolutions}{
    \begin{td-sol}\,
        \begin{enumerate}[label=(\alph*)] % chktex 36
            \item On a
            \begin{equation*}
                \begin{aligned}
                    \dpar{J}{x}(x,y) &= 2x - 3y^2,\\
                    \dpar{J}{y}(x,y) &= 4y^3 - 6xy.
                \end{aligned}
            \end{equation*}
            Les points critiques sont les solutions du système
            \begin{equation*}
                \begin{cases}
                    2x - 3y^2 &= 0,\\
                    4y^3 - 6xy &= 0.
                \end{cases}
            \end{equation*}
            On résout le système pour obtenir \(x = 0\) et \(y = 0\). Donc le seul point critique est \((0, 0)\).

            \item On a
            \begin{equation*}
                \begin{aligned}
                    J(t d_1, t d_2) &= {(t d_2)}^4 - 3(t d_1){(t d_2)}^2 + {(t d_1)}^2\\
                    &= t^4 d_2^4 - 3t^3 d_1 d_2^2 + t^2 d_1^2\\
                    &= t^2 (t^2 d_2^4 - 3t d_1 d_2^2 + d_1^2).
                \end{aligned}
            \end{equation*}
            On a donc
            \begin{equation*}
                J(t d_1, t d_2) = t^2 J(d_1, d_2).
            \end{equation*}
            Donc \(J(t d_1, t d_2)\) est un polynôme de degré 2 en \(t\) et donc son minimum est atteint en \(t = 0\).
            Donc \((0, 0)\) est un minimum local le long de toute droite passant par \((0, 0)\).
            %%%%%%%%% A REFAIRE
            ATTENTION
            %%%%%%%%% A REFAIRE
            etudier la double dérivée pour montrer sa convexité (ou passer par un equivalent)
            qui montre que c'est un minimum local

            \item On a \(x = y^2\) et donc
            \begin{equation*}
                J(y^2, y) = y^4 - 3y^4 + y^4 = -y^4.
            \end{equation*}
            Donc \((0, 0)\) est un maximum local de la restriction de \(J\) à la parabole d'équation \(x = y^2\).

            \item On calcule la matrice hessienne de \(J\):
            \begin{equation*}
                \begin{aligned}
                    \dpar{^2 J}{x^2}(x,y) &= 2,\\
                    \dpar{^2 J}{y^2}(x,y) &= 12y^2 - 6x,\\
                    \dpar{^2 J}{x y}(x,y) &= -6y.
                \end{aligned}
            \end{equation*}
            Donc
            \begin{equation*}
                J''(0,0) = \begin{pmatrix}
                    2 & 0\\
                    0 & 0
                \end{pmatrix}.
            \end{equation*}
            On a une valeur propre nulle donc on ne peut pas conclure sur la nature du point critique \((0, 0)\)
            directement avec la matrice hessienne. Mais en utilisant les questions \((b)\) et \((c)\), on peut
            dire que \((0, 0)\) est un point selle.

        \end{enumerate}
    \end{td-sol}
}{}